\section{Related work}
\label{sec:relatedwork}

This section provides an overview of existing research related to this work, structured into three main areas: simulation environments, misinformation with LLMs and opinion dynamics.

\subsection{Simulating Social Networks}

Social simulations have been developed as a tool to study the behavior of groups of people, addressing the challenges posed by the nonlinear effects of individual interactions \cite{squazzoni2014socialsimulation}.
Agent-Based Modeling (ABM) focuses on the dynamics of agents at the local level, and shows that these simple interactions can recreate complex social phenomena and group behavior \cite{macy2002abm}.
Specifically, ABM allows researchers to relate social phenomena at both micro and macro level, showing the causal relation between individual behavior and the structural properties of the network. \cite{squazzoni2014socialsimulation}.
The main limitations historically highlighted are that individuals follow simple rules \cite{conte2014agent}, and that they lack the capacity to reason and engage in social interaction.\cite{törnberg2023evaluate}
However, the recent advancements in LLMs offer a new opportunity to overcome the drawbacks of classical ABM systems, as they enable the generation of agents capable of engaging in realistic conversations and reproducing believable human-like behavior \cite{park2023genagents}.

Several recent works have explored the potential of LLM agents in social network simulated environments. Below, we discuss three relevant simulators.
\medskip

% Törnberg
\citet{törnberg2023evaluate} simulated three social media platforms, each with a specific content recommendation algorithm, to evaluate how alternative news feed algorithms impact the quality of online conversation, while increasing the interaction between opposing views.

The agents in the simulation, powered by LLMs, are individuals initialized with demographic characteristics, political leaning, interests and attitudes, taken from the 2020 American National Election Study (ANES).
The first platform promotes the most popular posts by following users, while the second one suggests the globally most popular posts. Both algorithms resulted in reduced cross-party interactions and increased toxicity.
In contrast, the third system introduces a "bridging" algorithm, which recommends the posts popular among users with opposing political views. The result is that the interactions were the least toxic, more constructive, and with more inter-partisan interactions.

This finding highlights the direct impact that content recommender systems have on the quality of online discourse.
\medskip

% S3
In the system proposed by \citet{gao2023s3socialnetworksimulationlarge}, the LLM agents keep a memory pool with the most relevant content they posted. This enhances the agents' cognitive coherence and realism, making their behavior and attitudes more realistic.
The system was then evaluated using real-world social network data on two levels: the considered The system was evaluated using real-world social network data on two levels: at the individual level, aspects such as emotions, attitude, and content generation were considered; at the population level, the focus was on information propagation, and the spread of emotions and attitudes. 
Results show that the system achieved promising accuracy, and that it was able to reproduce complex real-world trends.

\medskip
% YSocial
\citet{rossetti2024ysocialllmpoweredsocial} introduced Y, a social media digital twin, a system designed to digitally replicate a real-world system to allow analysis, simulation and experimentation in a controlled environment.
The users of these simulations are LLM agents, and they can perform all the common actions available on the most popular social media, including posting, commenting, replying, reacting and following other users. Other modules also allow the integration of images.
User profiles are enriched with attributes including their interests, political leaning, demographic data and personality, which is defined according to the Big Five model \cite{barrick1991bigfive, McCrae1992}.
To make the simulations even more realistic, Y also includes the possibility of adding external input to the simulation. Specifically, users can share news gathered from selected websites, provided through RSS (Really Simple Syndication) feeds.
Moreover, Y includes the implementation of various recommender and ranking algorithms to promote specific content or users. This enables further study of the impact the algorithmic curation has on online conversations and users' behavior. This expands the approach of \citet{törnberg2023evaluate}, by offering a more flexible and realistic simulation framework.



\subsection{Simulating misinformation with LLM agents}
Rumor dissemination has long existed, even with traditional communication platforms such as newspapers, radio, or television. However, the raise of online social networks has dramatically increased both the speed and scale at which fake news and misinformation can spread \cite{aimeur2023fake}. These platforms allow information to propagate almost instantly across large networks of users. As a result, misleading or entirely false content can reach large audiences before it is even identified or corrected.

\medskip
Many studies have attempted to model and understand this phenomenon with traditional Agent-Based Modeling (ABM) systems \cite{gausen2021can, sulis2020, muhammad2024agent}. In the traditional approach, agents behave according to fixed rules. While useful, this systems are limited, because they lack the richness and complexity of human communication. 

\medskip
More recently, researchers have begun to explore the use of LLMs as agents within these simulations, to leverage their abilities to emulate human-like reasoning and content generation.
These capabilities allow LLM agents to participate in improved interactions, making the simulations more realistic.

Indeed, LLMs are able to produce high-quality content even in the context of disinformation campaigns, producing text that appears convincingly human. A study by \cite{williams2025hqdisinformation} has shown that their content is undistinguishable from human-generated content over 50\% of the time, raising concerns about their potential role in amplifying misinformation.

Several recent works have begun to investigate on the dual role of LLM agents: they can either contribute or mitigate misinformation diffusion.

\medskip
The study of \citet{hu2025simulatingrumorspreadingsocial} highlights that the network structure and the individual personalities of agents have a direct influence on how misinformation propagates. This suggests that both macro-level (network) and micro-level (individual traits) factors are important in shaping the outcome.

\medskip
The system proposed by \citet{liu2024tinyslipgiantleap} assigns specific roles to LLM agents, determining how they interact with the information they consume and produce, with a particular emphasis on fake news. 
The possible roles are the following: 

\begin{itemize}
    \item \textit{Spreaders} actively disseminate information
    \item \textit{Verifiers} check the accuracy of the content
    \item \textit{Commentators} engage with content and express their opinions
    \item \textit{Bystanders} passively observe the information
\end{itemize}

This role-based framework allows a more detailed modeling of user behavior withing the system.
Moreover, their findings also revealed that political fake news tend to spread more rapidly than misled content on other topics, highlighting the importance and the risks of disinformation associated with political discourse.



\subsection{Opinion Dynamics}
One of the biggest challenges in simulating social behavior is modeling opinion dynamics. This field of social sciences has traditionally focused on mathematical models to describe the change and evolution of individuals' opinions when interacting.
\medskip

% Mathematical models
The DeGroot model \cite{Degroot1974} is a widely used model which is based on the idea that individuals are susceptible to other people's opinions. For this reason, each opinion is modeled as a weighted average of the neighbors' opinions.
The extension proposed in the Friedkin-Johnsen model \cite{friedkin_1990} adds a term to consider the "stubbornness" of the individual toward their initial idea. 
In another variant, the opinion of individuals is influenced by both their own current opinion and that of the neighbors \cite{Ye2018Opinion, Liu_2018}. This discrete-time solution is equivalent to the Friedkin-Johnsen model, but the stubbornness is on the current opinion, rather than the initial one.

\medskip
% LLMs
While mathematical models offer valuable insights, they represent behavior by abstracting many of the complexities of human behavior, which is shaped by both external influences and internal traits such as personality.

For this reason, recent works started to experiment with LLM agents' capabilities in realistically describing the evolution of individual opinions. 
Not only do they have the ability to impersonate a given profile in interactions with other individuals, but they can also express their beliefs in a textual format.
This allows more realistic simulations and makes it easier for human interpretation.

The agents in the experiment presented by \citet{cau2025languagedrivenopiniondynamicsagentbased} discussed in pairs about the Ship of Theseus paradox, chosen to prevent the convergence towards a specific direction.
The opinion is defined as a discrete value within the range 0-6, and it is updated with unitary steps depending on whether they were convinced by their partner.
The results show that LLMs tend to agree with a given statement and interacting partners. However, this study doesn't fully leverage LLM capabilities, as agents lack personality and demographic characterization.

In the work proposed by \citet{gao2023s3socialnetworksimulationlarge}, the attitude towards a topic is instead described by a Markov model in a binary spectrum. LLMs are initialized with a profile, and they evaluate the opinion transitions, also taking into account their current attitude and the messages received.

Agents in the study of \citet{chuang2024simulatingopiniondynamicsnetworks} interacted in a dyadic setting. Specifically, users reply to another user's post by explaining their opinion in textual format, which is then converted into a numerical score by an opinion classifier.
This study reveals that LLMs tend to converge towards accurate information, therefore introducing confirmation bias is essential to better resemble human behavior and opinion dynamics.

\citet{Liu_2024} simulated user interactions in presence of fake news. In this study, users are initialized with realistic personas and a memory model, and express their opinion in a tweet format. They are exposed daily to tweets from other random users, but there is no explicit modeling of the social graph, nor content distribution algorithm.

The research conducted by \citet{piao2025emergencehumanlikepolarizationlarge} showed that LLMs are capable of reproducing human behavior and phenomena. They also noticed that LLMs tend to reach a consensus on fact-based topics (such as the flat Earth theory), while they develop a polarization pattern on political topics.
To extract a numerical representation of the agents' opinions, they self-assess their political leaning, which is then converted to a numerical format.


