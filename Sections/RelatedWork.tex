\section{Related work}
\label{sec:relatedwork}

This section provides an overview of existing studies relevant for this work, to give a context of the current contributions in literature.
It is structured into three main areas: simulation simulating environments, the use of LLMs to generate and study misinformation, and the opinion dynamics modeling.

\subsection{Simulating social networks}

Social simulations have been developed as a tool to study the behavior of groups of people, addressing the challenges posed by the nonlinear effects of individual interactions \cite{squazzoni2014socialsimulation}.
Agent-Based Modeling (ABM) focuses on the dynamics of agents at the local level, and shows that these simple interactions can recreate complex social phenomena and group behavior \cite{macy2002abm}.
Specifically, the ABM approach allows to relate social phenomena observable at both micro and macro level, highlighting the causal relation between individual behavior and the structural properties of the network. \cite{squazzoni2014socialsimulation}.

Te main limitations historically highlighted of traditional ABMs concerns the simplicity of the rules \cite{conte2014agent} describing agents' behavior, and their inability to reason and realistically engage in social interaction.\cite{törnberg2023evaluate}
In the last years, however, the advancements in AI and LLMs offer a new opportunity to overcome these drawbacks, making it possible to generate agents capable of engaging in realistic conversations and reproducing believable human-like behavior \cite{park2023genagents}.

Several recent works have explored the potential of LLM agents in simulated social network environments. Below, we discuss three relevant simulators.

% Törnberg
\medskip
\citet{törnberg2023evaluate} simulated three social media platforms, each characterized by a specific content recommendation algorithm, in order to evaluate how alternative news feed personalization impact the quality of online conversation, while increasing the interaction between opposing views.

The agents in the simulation, powered by LLMs, are individuals initialized with demographic characteristics, political leaning, interests and attitudes, taken from the 2020 American National Election Study (ANES).

The first platform promotes the most popular posts by following users, while the second one suggests the globally most popular posts. Both algorithms resulted in reduced cross-party interactions and increased toxicity.
On the contrary, the third system introduces a "bridging" algorithm, suggesting posts which are popular among users with opposing political views. The result is that the interactions were the least toxic, more constructive, and with more inter-partisan interactions.

This finding highlights the direct impact that content recommender systems have on the quality of online discourse.

% S3
\medskip
In the system proposed by \citet{gao2023s3socialnetworksimulationlarge}, called $S^3$, the LLM agents are designed to keep a memory pool with the most relevant content they posted. 
This memory mechanism enables agents to keep  cognitive coherence and realism in their future interactions, making their behavior and attitudes more realistic over time.
Differently from other more simple models, where each action is independent from the previous ones, in $S^3$ the agents' decisions are influenced by their history, making their behavior similar to real-world users.

The system has been evaluated using real-world social network data, with a two-level analysis.
At individual level, the considered aspects were emotions, attitude, and content generation. At population level, instead, the study focused on information propagation in the network, and the spread of emotions and attitudes.

The results show that the system has achieved promising accuracy in the simulation, proving the possibility to replicate complex dynamics observable on real social network.
Specifically, the agents' behavior showed coherency with real-data emerging trends, highlighting the potential of LLM-based agents equipped with memory to provide reliable insights at both individual and system level


\medskip
% YSocial
\citet{rossetti2024ysocialllmpoweredsocial} introduced Y, a social media digital twin, a system designed to digitally replicate a real-world system to allow analysis, simulation and experimentation in a controlled environment.
The users of these simulations are LLM agents, and they can perform all the common actions available on the most popular social media, including posting, commenting, replying, reacting and following other users. Other modules also allow the integration of images.
User profiles are enriched with attributes including their interests, political leaning, demographic data and personality, which is defined according to the Big Five model \cite{barrick1991bigfive, McCrae1992}.
To make the simulations even more realistic, Y also includes the possibility of adding external input to the simulation. Specifically, users can share news gathered from selected websites, provided through RSS (Really Simple Syndication) feeds.
Moreover, Y includes the implementation of various recommender and ranking algorithms to promote specific content or users. This enables further study of the impact the algorithmic curation has on online conversations and users' behavior. This expands the approach of \citet{törnberg2023evaluate}, by offering a more flexible and realistic simulation framework.



\subsection{Simulating misinformation with LLM agents}
Rumor dissemination has long existed, even with traditional communication platforms such as newspapers, radio, or television. However, the raise of online social networks has dramatically increased both the speed and scale at which fake news and misinformation can spread \cite{aimeur2023fake}. These platforms allow information to propagate almost instantly across large networks of users. As a result, misleading or entirely false content can reach large audiences before it is even identified or corrected.

\medskip
Many studies have attempted to model and understand this phenomenon with traditional Agent-Based Modeling (ABM) systems \cite{gausen2021can, sulis2020, muhammad2024agent}. In the traditional approach, agents behave according to fixed rules. While useful, this systems are limited, because they lack the richness and complexity of human communication. 

\medskip
More recently, researchers have begun to explore the use of LLMs as agents within these simulations, to leverage their abilities to emulate human-like reasoning and content generation.
These capabilities allow LLM agents to participate in improved interactions, making the simulations more realistic.

Indeed, LLMs are able to produce high-quality content even in the context of disinformation campaigns, producing text that appears convincingly human. A study by \cite{williams2025hqdisinformation} has shown that their content is undistinguishable from human-generated content over 50\% of the time, raising concerns about their potential role in amplifying misinformation.

Several recent works have begun to investigate on the dual role of LLM agents: they can either contribute or mitigate misinformation diffusion.

\medskip
The study of \citet{hu2025simulatingrumorspreadingsocial} highlights that the network structure and the individual personalities of agents have a direct influence on how misinformation propagates. This suggests that both macro-level (network) and micro-level (individual traits) factors are important in shaping the outcome.

\medskip
The system proposed by \citet{liu2024tinyslipgiantleap} assigns specific roles to LLM agents, determining how they interact with the information they consume and produce, with a particular emphasis on fake news. 
The possible roles are the following: 

\begin{itemize}
    \item \textit{Spreaders} actively disseminate information
    \item \textit{Verifiers} check the accuracy of the content
    \item \textit{Commentators} engage with content and express their opinions
    \item \textit{Bystanders} passively observe the information
\end{itemize}

This role-based framework allows a more detailed modeling of user behavior withing the system.
Moreover, their findings also revealed that political fake news tend to spread more rapidly than misled content on other topics, highlighting the importance and the risks of disinformation associated with political discourse.



\subsection{Opinion Dynamics}
One of the major challenges in simulating social behavior is modeling opinion dynamics, i.e., the mechanism through which individuals' opinions evolve over time, especially through interactions with others. 
This field of social sciences has traditionally relied on mathematical models, which offer a formal abstraction of influence mechanisms among agents.

% Mathematical models
\medskip
Among the most well-established models is the DeGroot model \cite{Degroot1974}, which is based on the idea that individuals are susceptible to other people's opinions. For this reason, each opinion is modeled as a weighted average of the neighbors' opinions.
This basic form of social influence assumes full susceptibility to the surrounding environment, but fails to account for individual resistance to change.

To address this, the Friedkin-Johnsen model \cite{friedkin_1990} extends DeGroot by introducing the notion of "stubbornness", the agent's tendency to retain part of its initial opinion. 
This is formalized through a susceptibility parameter, which allows each agent to weight their original belief against the influence of others.

A further variant of these models considers state-dependent updating \cite{Ye2018Opinion, Liu_2018}, where agents adjust their beliefs based on their current opinion rather than their initial one.


% LLMs
\medskip
While mathematical models offer valuable insights, they  simplify complex human processes by reducing opinions to numerical values and ignoring factors such as language, emotional tone, or personality.

To overcome such abstractions, recent studies have begun to leverage LLMs as agents in simulations. LLM-based agents can realistically describe the evolution of individual opinions, since they have the ability to impersonate a given profile in interactions with other individuals, and they can also express their beliefs in a textual format.
These characteristics make them more suitable for capturing real-life mechanisms like opinion exchange, including bias, tone and context awareness.

The agents in the experiment presented by \citet{cau2025languagedrivenopiniondynamicsagentbased} discussed in pairs about the Ship of Theseus paradox, a topic selected precisely because it lacks a factual resolution, so it prevents the convergence towards a specific direction.
Each agent has an opinion, defined as a discrete value within the range 0-6, which can be updated with unitary steps depending on whether they are persuaded by their interlocutor.
The results show that LLMs tend to agree with a given statement and interacting partners. However, this study doesn't fully leverage LLM capabilities, as agents lack deeper personalization, such as demographic traits and personality profiles.

\citet{gao2023s3socialnetworksimulationlarge} introduced a more structured update mechanism, representing attitude evolution through a Markov model on a binary spectrum. 
LLM agents are initialized with predefined profiles, and they transition between belief states by evaluating received messages and their current attitudes.
This formalization links language-based input to structured state change.

Agents in the study of \citet{chuang2024simulatingopiniondynamicsnetworks} interact in a dyadic setting. Specifically, users reply to another user's post by explaining their opinion in textual format, which is then converted into a numerical score by an opinion classifier.
This study reveals that LLMs tend to converge towards accurate information. However, the authors also note that to better replicate human behavior, it is necessary to introduce confirmation bias, as humans often reinforce prior beliefs instead of updating them.

\citet{Liu_2024} simulated user interactions in a social media setting where agents are initialized with realistic detailed personas and equipped with memory modules.
These agents express their opinion in a tweet format. 
Each day, they are exposed to posts from other random users, allowing them to update their views.
While the simulation captures the dynamic exposure to content, it lacks an explicit representation of the underlying social graph, since message propagation doesn't follow a realistic network topology, nor is influenced by content distribution algorithms typical of online platforms.

Finally, the research conducted by \citet{piao2025emergencehumanlikepolarizationlarge} showed that LLMs are capable of reproducing human behavior and phenomena. 
Their results showed that LLMs tend to reach a consensus on fact-based topics (such as the flat Earth theory), whereas they develop a polarization pattern on political topics, closely resembling human social behavior.
In their system, agents assess their own political leaning by self-rating their beliefs, which are then converted to a numerical score, offering a consistent and interpretable mapping between language and opinion.

\medskip
These studies demonstrate that while mathematical models provide a foundational understanding of social influence, LLM-based agents bring new flexibility to opinion dynamic simulations, as they can integrate the characteristics of human communications.


