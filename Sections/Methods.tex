\section{Methods}
\label{sec:methods}

This section provides a description of the simulator and the implemented features.
First, it introduces the framework and its workflow.
Then, it details agent modeling, their initialization and behavior.
Finally, the implemented opinion models are described, including the impact of agent behavior on their opinions.

%% SIMULATION
\subsection{Simulation workflow}
This work is based on Y, a social media digital twin \cite{rossetti2024ysocialllmpoweredsocial}, designed to realistically simulate a social platform.
This system provides a modular structure, allowing the study of emerging behavior in a population of virtual agents, interacting in a platform similar to real-world social media.
Algorithm \ref{alg:workflow} describes the base flow of the simulations on which this work is based on.

\medskip
The proposed workflow preserves the core original implementation of Y, and extends with a further phase, in which agents update their opinions on the topics discussed during the day.
This extension enables a deeper analysis of the social dynamics due to opinion change.

The behavior of the simulated environment is completely configurable, as it allows the specification of many parameters, such as: the hourly activity, the algorithms for recommending contents and users, the level of misinformation agents, the language model, and others.
This approach makes the system highly flexible, enabling to model various scenarios.

\medskip
At the beginning of the simulation, a population of agents is initially generated.
These agents are not initially connected, therefore there is not a predefined social network.
However, throughout the simulation, agents have the possibility to create new links or remove the existing ones, evaluating the interactions they had with other users.
In this way, the network structure dynamically emerges, and it evolves over time according to the agents' behavior and interactions.

Each simulated day is composed of a set of rounds, corresponding to virtual hours.
In each round, a number of active agents is sampled, according to the hourly activity configured.
Agents can then perform an action: publish content, react, follow or unfollow other users, and eventually reply to previously received mentions.
The specific behavior of each agent depends on its profile, its personality and the content it's interacting with. 
An explanation of the possible actions is detailed in the next subsection.

At the end of the day, active agents are asked to update their opinion on the topics they discussed.
This phase is critical to study the opinion dynamics: it makes it possible to observe how social interactions and the received content impact the evolution of individual views.
The opinion is updated leveraging the linguistic skills of the LLM, as described in the dedicated subsection.

\medskip
The original framework also allows to dynamics manage a percentage of agents that can leave or join the platform daily. 
However, this work doesn't use this feature, as the goal is to focus on a stable set of users.
Therefore, the considered population is fixed for the whole simulated time frame, to prevent the turnover of users to significantly impact the results.

\begin{algorithm}
\caption{Simulation workflow}
\label{alg:workflow}
\begin{algorithmic}[1]
\COMMENTLINE{Agents creation and initialization}
\STATE $agents \gets create\_population()$

\COMMENTLINE{Simulation loop}
\FOR{$day \in n\_days$}
    \FOR{$round \in n\_rounds$}
        \COMMENTLINE{Sample agents active in the current round}
        \STATE $n\_actives \gets len(agents) \times hourly\_activity[round]$
        \STATE $active\_agents \gets sample(agents, n\_actives)$ 
        \FOR{$agent \in active\_agents$}
            \COMMENTLINE{Perform actions}
            \STATE $agent.select\_action()$
            \STATE $agent.reply\_mentions()$
        \ENDFOR
    \ENDFOR

    \COMMENTLINE{Add new connections}
    \STATE $sel\_agents \gets sample(daily\_actives, percentage\_daily\_follows)$
    \FOR{$agent \in sel\_agents$}
        \STATE $agent.search\_and\_follow()$
    \ENDFOR

    \COMMENTLINE{Opinion update}
    \FOR{$agent \in daily\_actives$}
        \STATE $agent.update\_opinions()$
    \ENDFOR

\ENDFOR
\end{algorithmic}
\end{algorithm}



\subsection{Agents}
% Intro
One of the biggest challenges in social simulations is to realistically model agents and their behavior.
In the following, there is a description of the agent initialization phase and their behavior modeling in the context of this work. 
Then, an explanation of a new category of agents, diffusing misinformation, is provided.


%% AGENT INIT
\subsubsection{Initialization}
The agents in Y framework are initialized with various profile features, which contributes to the creation of virtual users with a detailed description and characterization.

% Random data and personality
Some profile dimensions are randomly sampled: name, surname, email, password and personality.
Specifically, the personality is defined according to the Five-Factor Model \cite{barrick1991bigfive, McCrae1992}, mostly known as the Big Five model. 
Users can be either high or low in each dimension, as described Table \ref{tab:susceptibility}, which leads to at most 32 combinations of distinct personalities.

\medskip
% Age and gender
In this study, the age and gender of agents are randomly assigned, but according on a weighted distribution based on 2024 Twitter statistics \cite{statista2024twitter}. 
Only the data for people aged 18 and above were considered, and the maximum age is set to 60, in line with the value in the original configuration of Y.

% Nationality and interests
Moreover, all agents are set with Italian nationality, in order to guarantee coherency with the context of the presented case study.
All agents are also configured with the same four interests, corresponding to the topics analyzed in this study: \textit{Civil rights}, \textit{Immigration}, \textit{Nuclear energy}, \textit{Reddito di Cittadinanza}.

These topics were selected because they represent politically relevant issues in the Italian context of 2022, on which the main coalitions held different positions.
This allows the simulation to generate meaningful political discussions and potential conflicts among agents.
Furthermore, these issues don't have a single ground truth, meaning that the simulations do not necessarily lead to consensus \cite{cau2025languagedrivenopiniondynamicsagentbased}.


\medskip
% Attributes from real data
To make the users even more realistic, some attributes have been initialized starting from real-world data, based on the dataset presented by \citet{pierri2023ita}. 
This includes Twitter posts collected around the Italian political elections in 2022.
Specifically, the attributes initialized from the dataset in this work are: the political leaning, the toxicity in writing posts and comments, and the activity level, for each user.
The activity is computed by converting the number of tweets posted by each user into a continuous value in the range $[0,1]$, with a logaritmic normalization to reduce the impact of outliers.
The formula used is the following:

\[
activity_x = \min\left( \frac{\log(1 + n\_posts_x)}{\log(1 + N_{99.5})},\ 1.0 \right)
\]

where $n\_posts_x$ is the number of posts written by user $x$, and $N_{99.5}$ is the 99.5th percentile.


% Agent roles and prompts
\medskip
The orchestration of agents  in this simulator has been enabled by AutoGen \cite{pyautogen0.2.31, wu2023autogenenablingnextgenllm}, which allows multi-agent conversation.
LLM-agents are initialize with their full profile, before performing any action in the system. 

The prompt to initialize the agent with its role is the following: 
\input{prompts/agent_roleplay}

% Topic descriptions
\medskip
The agent role initialization also includes a description of the topics considered.
This is done for two reasons: to provide the LLM with the necessary background knowledge (especially regarding the 2022 Italian political views), and also to outline the meaning of the stances associated with each topic.
This enables consistency and clarity in the simulation, as clarifying what it means to be supporting or opposing a topic is crucial for guaranteeing a coherent opinion update.

The following are the provided descriptions for each topic:

\begin{itemize}
    \item \textbf{Civil rights}:
        \begin{tcolorbox}[prompt]
            Covers gender equality, LGBTQIA+ rights and family structure. Supporters support expanding protections for LGBTQIA+ individuals, gender equality, and inclusive definitions of family; opponents prioritize traditional family models and may reject changes to marriage, parenting, or gender roles.
        \end{tcolorbox}

    \item \textbf{Immigration}:
        \begin{tcolorbox}[prompt]
            Debates focus on border control, bilateral agreements, and managing irregular migration. Supporters advocate for inclusive immigration policies, humanitarian protection and integration; opponents prioritize national security and strict border enforcement.
        \end{tcolorbox}

    \item \textbf{Nuclear energy}:
        \begin{tcolorbox}[prompt]
            Debates focus on whether to include it in the energy mix. Supporters cite energy security; opponents stress risks, costs, and favor renewables.
        \end{tcolorbox}

    \item \textbf{Reddito di cittadinanza}:
        \begin{tcolorbox}[prompt]
            A state subsidy for people living in poverty, designed to ensure a minimum standard of living and promote employment integration. Supporters believe reddito di cittadinanza is a necessary tool for social protection and inclusion; opponents are concerned about potential work disincentives and system abuses. The most radical want to abolish it, others aim to reform it.
        \end{tcolorbox}    
\end{itemize}


% Coalition opinions
Furthermore, the opinion of the supported coalition is provided for every topic, and it includes a label summarizing the stance (supportive/opposed), followed a more detailed textual description.

The description of the political views have been adapted from \cite{wired2022elezioni12punti, ilpost2022partiti, pagellapolitica2022confronto, pagellapolitica2022programmi}.
To provide an example, the opinions for the \textit{Centre-Left} coalition are the following:

\input{Coalitions/Centre-Left_opinions}

A full list of the political opinions for each coalition can be found in Appendix \ref{app:coalition_opinions}.



%% BEHAVIOR
\subsubsection{Agents' behavior}
When an agent is active, one among the following actions is performed:
\begin{itemize}
    \item \textbf{Post}: writing a tweet about a given topic.
    \item \textbf{Comment}: after reading a given conversation, the agent is asked to comment a post; he can then choose to add a reaction (\textit{like} or \textit{dislike}), and \textit{follow} or \textit{unfollow} the author of the post.
    \item \textbf{Read}: the agent reads a given tweet without adding any contribution to the conversation; however, he can decide to add a reaction (\textit{like} or \textit{dislike}) and \textit{follow} (or \textit{unfollow}) its author.
\end{itemize}

\medskip
% Select action
In the original implementation of Y, the agents were responsible of choosing which action to perform.
In this work, instead, the agent behavior is driven by two activity values, each in the range $[0,1]$: one concerns the agent's tendency to post, the other to comment. 
The remaining probability, if their sum is less then 1, is automatically assigned to the \textit{read} action.
This modification allows to model the agent behavior in a more realistic way, leveraging the real data available in this study, as agents tend to behave coherently with the activity levels observed in real data.

All the prompts with the instructions to perform an action can be found in Appendix \ref{app:prompt_actions}.

% How topics are chosen (get_interest) + language
When an agent needs to write a post, the provided prompt specifies the topic, which is chosen among the active interests of the user.
Specifically, it is randomly chosen among the candidate topics, which are the ones that have been active in the configured time window.
Although the agents represent Italian users and topics, all generated content in the simulation is in English, reflecting the default language setting of the simulation environment.

% Content recsys
When agents interact with content published by other users (by reading or commenting), the recommended posts are selected by a recommendation system.
Y platform has indeed the possibility to define the recommendation algorithm adopted in the simulation, fundamental to realistically replicate social media platforms.
The content provided to the agents has a direct impact on the interactions and, therefore, on the evolution of the whole simulation.
Among the various algorithms proposed by the framework, the following have been adopted in this work:
    \begin{itemize}
        \item \textit{ReverseChronoFollowersPopularity}: recommends recent content from followed users, sorted by their popularity. A specified percentage of content comes from non-followed users, to guarantee exposure to different views.
        \item \textit{ContentRecSys}: randomly selects a subset of the content published on the platform.
    \end{itemize}

% User recsys
For the same reason, the framework also supports various algorithms for recommending users to follow. 
The one adopted in this work is the default one, \textit{PreferentialAttachment}, which recommends users based on a score computed as the product of the agent’s neighbor set size and that of the candidate user.


%% MISINFORMATION
\subsubsection{Misinformation agents}
This work extends the agents modeling proposed by Y, by introducing a special category of agents, specifically designed to generate misinformation.
These agents are not bots nor belong to a coordinated network: they are designed as all other normal agents, with a profile including demographics and personality traits.

% Misinformation content
The main difference, with respect to other users, is that they are asked to generate misleading content supporting their views.
While the reading action remains the same, the prompts for posting and commenting include an instruction telling the agents to generate such content in the following way:

\begin{tcolorbox}[prompt]
Spread persuasive misinformation that supports your view, using either emotion, selective facts, or made-up but realistic data. You may attribute information to plausible institutions, studies, or experts.
\end{tcolorbox}

Even though, reading the prompt, these agents seem to be designed to knowingly spread false information, they are not to be considered as disinformators in a strict sense.
There is no explicit harmful intent, and they are not part of a structured organization or of a planned disinformation campaign.
For this reason, in the context of this study, they are considered as misinformators, individuals that share inaccurate or misleading content, but with no malicious intent.

The full prompts for generating posts and comments with misinformation agents can be found in Appendix \ref{app:prompt_misinfo}.

\medskip
% Coalition assignment, toxicity and activity generation
Another difference with respect to other agents is that these users are not initialized with the data of real users.
For instance, the supported political leaning is randomly assigned, in order to obtain a uniform distribution among the various coalitions.

Even the toxicity level of the content they write is not directly assigned with the data of a real user.
This value has been model with a process based on statistical distributions.
Specifically, for each coalition, the best-fitting distribution is identified, analyzing real data, among beta, log-normal, power-law and gamma distributions.
Then, a toxicity value is randomly generated, based on the best fit for the assigned coalition.

The same approach has been used to determine the agents' activity, starting from the number of posts and comments.
Since these variables represent discrete quantities, they have been modeled using distributions like the Poisson or negative binomial distributions, fitted on the observed data.
The resulting values are then converted into normalized activity scores, consistently with what is done for other agents in the simulation.


%% OPINION
\subsection{Opinion modeling and update}
One of the crucial aspects on which this work focuses on is opinion dynamics.
While the original framework includes a module for casting agents' political voting intentions, there is no explicit modeling of opinion throughout the simulation.
Providing individuals with their views in a textual format ensures a coherent behavior at individual level, resulting in an enhanced and more realistic representation of the population.

% How opinion is modeled
\medskip
In this work, the opinion is modeled as a numerical score that ranges from $-1$ (strongly opposed) to $+1$ (strongly supported), and is associated with a corresponding textual explanation.
The initial opinions of agents correspond to those of their supported political coalition, and it can evolve over time.

% Intro to model categories
\medskip
The models implemented in this work can be grouped into two categories.
The first one includes some basic score aggregation and various known opinion models, in order to mathematically represent the evolution of agents' views.
The second solution employs LLMs, leveraging their ability to impersonate the user, to reason, and to express their opinions in text.


%% TRADITIONAL MODELS
\subsubsection{Mathematical modeling}
% Median, weighted mean
A straightforward approach to estimate an individual's opinion is to compute the median or the weighted mean of previous obtained scores.
This is particularly useful when opinions are externally assigned, and a summary representation is needed.

\medskip
% Friedkin-Johnsen
A model implemented in this work is the one presented by \citet{friedkin_1990}, which considers both the individuals' initial opinion and those of their neighbors, weighted by a susceptibility value:
\[
x_i(t + 1) = (1 - \lambda_i) x_i(0) + \lambda_i   \sum_{j \in N_i(t)} w_{ij} x_j(t)
\]
where $x_i(t)$ is the opinion of individual $i$ at time $t$, $N_i$ is the set of following users, $\lambda_i$ is the user's susceptibility to other users, and $w_{ij}$ is the impact user $j$ has on $i$. 

Note that in this first implementation only the followed users are considered neighbors, and they all have the same weight.

% Susceptibility score
The susceptibility $\lambda$ is a value in the range $[0,1]$, $0$ meaning \textit{not susceptible}, $1$ \textit{highly susceptible}.
This approach relies on the idea that personality traits have an impact on social influence \cite{oyibo2019personality}, and it's therefore here computed for each individual as the mean of the scores for each assigned trait, visible in Table \ref{tab:susceptibility}.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|c|}
\hline
\textbf{Trait} & \textbf{Trait level} & \textbf{Description} & \textbf{Susceptibility score} \\
\hline
\multirow{2}{*}{Neuroticism}       
  & High & sensitive/nervous         & 0.9 \\
  & Low & resilient/confident        & 0.1 \\
\hline
\multirow{2}{*}{Openness to experience}          
  & High & inventive/curious         & 0.2 \\
  & Low & consistent/cautious        & 0.6 \\
\hline
\multirow{2}{*}{Conscientiousness} 
  & High & efficient/organized       & 0.2 \\
  & Low & extravagant/careless       & 0.6 \\
\hline
\multirow{2}{*}{Extroversion}      
  & High & outgoing/energetic        & 0.5 \\
  & Low & solitary/reserved          & 0.5 \\
\hline
\multirow{2}{*}{Agreeableness}     
  & High & friendly/compassionate    & 0.5 \\
  & Low & critical/judgmental        & 0.5 \\
\hline
\end{tabular}
\caption{The table shows the susceptibility scores assigned to each personality trait. Values range from $0$ (not susceptible) to $1$ (highly susceptible).}
\label{tab:susceptibility}
\end{table}

\medskip
% State-dependent Friedkin-Johnsen
A state-dependent version of the previous model considers the influence of the current opinion rather than the initial one \cite{Ye2018Opinion}:
\[
x_i(t + 1) = (1 - \lambda_i) x_i(t) + \lambda_i  \sum_{j \in N_i(t)} w_{ij} x_j (t)
\]

Equivalently to the previous solution, all followed users are assigned the same weight.

Alternatively, it's possible to also include in the set of neighbors all users the agent has interacted with since the last opinion update, through reactions. In this case, their weights depend on the type (and frequency) of the interactions, where the scores of a single interaction, later normalized, are assigned as follows: 
\begin{itemize}
    \item \textbf{follow}: $+1$
    \item \textbf{like}: $+0.2$
    \item \textbf{dislike}: $-0.2$
\end{itemize}

% Disclaimer: mathematical opinion score has no impact on the simulation
\medskip
It's important to clarify that this mathematical representation only concerns opinion values, and is not used inside the simulation, so it has no direct impact on the agents' behavior.
All the actions made by the users, including following another user and reacting to a post, are exclusively determined by the LLM-based agents, which are the only responsible for the evolution of the simulation.
Even the opinions provided back to the agents are generated by the LLMs, who evaluated their own updated scores (see explanation in the following paragraph), and don't rely on the scores obtained mathematically, which have been produced only for analysis and comparison purposes.


%% LLMs
\subsubsection{LLM-based opinion update}
Another solution implemented in this work to model the opinion dynamics is to directly ask LLM agents to update the opinion of the users they are impersonating.

Agents are initialized with the opinions of their supported political leaning. During the simulation, they act coherently with their ideas, and finally evolve their own views according to their daily interactions.

Agents are given their profile, the description of the topics to update, their supported coalition believes and their own current ideas on the topics, and a the memory of their actions since last opinion update.
Specifically, the memory tracks the posts a user reads and writes, the replies, the reactions, and whether there are changes in the follow status with another user, after interacting. 

With all this knowledge, the LLMs are prompted to provide their updated opinion in a textual format.
They are also asked to assign a stance to each of the updated topics, choosing among a set of given labels. This allows to easily extract a numerical score, according to the conversion visible in Table \ref{tab:stance}, and is useful for later analysis.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Stance} & \textbf{Score} \\
\hline
STRONGLY OPPOSED   & $-1.0$ \\
OPPOSED            & $-0.5$ \\
NEUTRAL            & $0$ \\
SUPPORTIVE         & $+0.5$ \\
STRONGLY SUPPORTIVE & $+1.0$ \\
\hline
\end{tabular}
\caption{Mapping agents' stance from textual labels to numerical scores.}
\label{tab:stance}
\end{table}

% Bias instruction
\medskip
The prompt to update the opinion also includes a confirmation bias, which is the cognitive tendency of people to interpret information only partially, favoring those coherent with the personal views or expectations \cite{nickerson1998}.
This allows to introduce a resistance to change, typical of human behavior, and it also guarantees a greater opinion fragmentation, as discussed in previous studies \cite{chuang2024simulatingopiniondynamicsnetworks}.

The presented study proposes two levels of bias, in order to to differentiate the behavior of misinformation agents from the rest of the population.
The base bias, introduced by \citet{Liu_2024}, is the following:

\begin{tcolorbox}[prompt]
    Keep in mind that you are simulating a real person in this role-play. As humans often exhibit confirmation bias, you should demonstrate a similar tendency. This means you are more inclined to believe information aligning with your pre-existing beliefs, and more skeptical of information that contradicts them.
\end{tcolorbox}


Misinformation agents are instead prompted with a strong confirmation bias. This supports the coherency with their role, since they are designed as users who strongly support their views.
The instruction, presented by \citet{chuang2024simulatingopiniondynamicsnetworks}, is shown below:

\begin{tcolorbox}[prompt]
    Remember, you are role-playing as a real person. You have a strong confirmation bias. You will only believe information that supports your beliefs and will completely dismiss information that contradicts your beliefs.
\end{tcolorbox}

The full prompt for updating the opinion can be found in Appendix \ref{app:prompt_opinion}.

This strategy allows to model in a more realistic way the evolution of opinions, simulating the complexity of human reasoning and the cognitive dynamics that influence the change of ideas over time.