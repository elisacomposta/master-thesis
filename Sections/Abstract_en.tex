Online social networks are often studied to analyze both individual and collective phenomena. 
In this context, simulators are widely used tools for exploring controlled scenarios. 
The integration of Large Language Models (LLMs) enables the creation of more realistic simulations, thanks to their ability to understand and generate content in natural language.

This work investigates the behavior of LLM-based agents in a simulated social network.
Agents are initialized with realistic profiles and are calibrated on real-world data, collected around the 2022 Italian political elections.
An existing social media simulator is extended by introducing mechanisms for opinion modeling and misinformation generation.
The aim is to examine how LLM agents simulate online conversations, interact, and evolve their opinions under different scenarios.

Results show that LLM agents can generate coherent content and establish connections with other users, building a realistic social network structure. 
However, the tone of their generated contents is less heterogeneous than the one observed in real data, in terms of toxicity.
The evolution of opinions assessed by LLMs evolves over time similarly to what is observed with traditional opinion models.
The exposure to misinformation content has no significant impact, suggesting that LLMs need more careful cognitive modeling in the initialization phase, to better replicate human behavior.
Another limitation of the study concerns the simulated time, which prevents from observing long-time effects such as the impact of the different recommendation algorithms.

Overall, LLMs demonstrate potential as tools for simulating user behavior in social environments, but challenges remain in capturing heterogeneity and more complex patterns.